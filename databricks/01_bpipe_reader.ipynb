{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb86ace1-b3da-4a1d-a054-dd0d0797c4f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# B-Pipe Spark Wrapper\n",
    "In this notebook, we will cover a few service names from B-Pipe and map them to a spark / spark streaming applications. For testing purpose, we use an [emulator](https://github.com/Robinson664/bemu) that must be available as part of your cluster libraries. Please note that we could not validate this approach against live B-Pipe feed, hence limiting the scope of this exercise to `//blp/refdata` and `//blp/mktdata` service names only. For the purpose of this exercise, let's create a synthetic portfolio of 6 US and London based equities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51bf2b0-6d6d-4b72-b488-8db6adba9a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "portfolio = [\n",
    "  [\"AMZN US Equity\", 1000],\n",
    "  [\"VOD LN Equity\", 599],\n",
    "  [\"AAPL US Equity\", 823],\n",
    "  [\"MSFT US Equity\", 122],\n",
    "  [\"BARC LN Equity\", 1321]\n",
    "]\n",
    "\n",
    "securities = [p[0] for p in portfolio]\n",
    "portfolio_df = pd.DataFrame(portfolio, columns=['security', 'shares'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d06c13-8477-4137-8a5b-c80448e87514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Historical Data Request\n",
    "\n",
    "`HistoricalDataRequest` is a specific request type sent via Bloomberg’s BLPAPI (Bloomberg’s API used with B-Pipe and other Bloomberg services). It asks for time series of historical data - for example, daily closing prices of Apple stock for the past 6 months, or historical yields for a government bond. We mapped its service again spark application accepting below parameters.\n",
    "\n",
    "**Required fields:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`fields`|`List[String]`|-|fields we want to return for our given securities. See list of supported field from [DATA\\<GO\\>](https://data.bloomberg.com/)|\n",
    "|`securities`|`List[String]`|-|list of securities|\n",
    "|`startDate`|`Date`|-|Starting date we want to get history from, formatted as YYYY-MM-dd|\n",
    "\n",
    "**Following options are supported:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`endDate`|`Date`|`NOW`|Ending date we want to get history to, formatted as YYYY-MM-dd|\n",
    "|`periodicityAdjustment`|`Option[String]`|`NONE`|Must be valid periodicity adjustment, like `ACTUAL`, `CALENDAR` or `FISCAL`|\n",
    "|`periodicitySelection`|`Option[String]`|`NONE`|Must be valid periodicity selection, like `DAILY`, `WEEKLY`, `MONTHLY`, etc.|\n",
    "|`pricingOption`|`Option[String]`|`NONE`|`PRICING_OPTION_PRICE` or `PRICING_OPTION_YIELD`|\n",
    "|`overrideOption`|`Option[String]`|`NONE`|`OVERRIDE_OPTION_GPA` or `OVERRIDE_OPTION_CLOSE`|\n",
    "|`adjustmentNormal`|`Option[String]`|`NONE`||\n",
    "|`adjustmentAbnormal`|`Option[String]`|`NONE`||\n",
    "|`adjustmentSplit`|`Option[String]`|`NONE`||\n",
    "|`maxDataPoints`|`Option[Int]`|`NONE`||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1307ec-7cd8-4c3e-b72f-31b3e96defb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given distributed nature of spark, one can specify the number of partitions we want to distribute this request against. In this case, each partition will be responsible for a specific B-Pipe request against a subset of securities provided. Suboptimal in specific cases where portfolio is made of securities of different liquidity (different traded volumes), this might remove the bottleneck of streaming an entire portfolio through 1 single request. See next notebook for more information about imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c857ce-f521-4356-8cf4-0ebafddd9d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hist_df = (\n",
    "  spark\n",
    "    .read\n",
    "    .format(\"//blp/refdata\")\n",
    "    .option(\"serviceName\", \"HistoricalDataRequest\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"fields\", \"['BID', 'ASK']\")\n",
    "    .option(\"securities\", securities)\n",
    "    .option(\"startDate\", \"2022-01-01\")\n",
    "    # naive partitioning by securities\n",
    "    .option(\"partitions\", 10)\n",
    "    .load().toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465649cf-8204-45b0-8dd9-5e2f6a809e83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(hist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f565bde-a0e3-4e85-86b9-b3293d48e4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "hist_df['SPREAD'] = hist_df['BID'] - hist_df['ASK']\n",
    "df = hist_df.pivot(index='TIME', columns='SECURITY', values='SPREAD')\n",
    "fig = px.area(df, facet_col=\"SECURITY\", facet_col_wrap=2, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194831c7-69db-4373-b3f6-5082b51e2d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reference Data Request\n",
    "\n",
    "ReferenceDataRequest is a type of API call (using Bloomberg’s BLPAPI) that asks for metadata or static attributes about securities — things that generally do not change tick-by-tick, such as Security descriptions, ISINs, CUSIPs, SEDOLs, Exchange codes, Sector classifications, etc. We mapped its service again spark application accepting below parameters.\n",
    "\n",
    "**Required fields:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`fields`|`List[String]`|-|fields we want to return for our given securities. See list of supported field from [DATA\\<GO\\>](https://data.bloomberg.com/)|\n",
    "|`securities`|`List[String]`|-|list of securities|\n",
    "\n",
    "**Following options are supported:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`overrides`|`Map[String, String]`|`EMPTY`|See list of overrides from [DATA\\<GO\\>](https://data.bloomberg.com/)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6f125a-0d96-48d3-9575-73d651cc46f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given distributed nature of spark, one can specify the number of partitions we want to distribute this request against. In this case, each partition will be responsible for a specific B-Pipe request against a subset of securities provided. Suboptimal in specific cases where portfolio is made of securities of different liquidity (different traded volumes), this might remove the bottleneck of streaming an entire portfolio through 1 single request. See next notebook for more information about imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44141589-6d45-4480-9950-15e41024db1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ref_df = (\n",
    "  spark\n",
    "    .read\n",
    "    .format(\"//blp/refdata\")\n",
    "    .option(\"serviceName\", \"ReferenceDataRequest\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"fields\", \"['PX_LAST','BID','ASK','TICKER','CHAIN_TICKERS']\")\n",
    "    .option(\"securities\", securities)\n",
    "    .option(\"overrides\", \"{'CHAIN_PUT_CALL_TYPE_OVRD':'C','CHAIN_POINTS_OVRD':'4','CHAIN_EXP_DT_OVRD':'20141220'}\")\n",
    "    # naive partitioning by securities\n",
    "    .option(\"partitions\", 5)\n",
    "    .load().toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6db74f-8fab-4b09-b0bc-8438fb1c6a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da45956-755b-4baa-b142-d7530f2df97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Intraday Tick Request\n",
    "\n",
    "A B-Pipe IntradayTickRequest is a type of request sent through Bloomberg B-Pipe to retrieve tick-by-tick historical data — meaning individual trades, bids, asks, or quote changes — over a specific short time window (typically minutes to hours within a day). We mapped its service again spark application accepting below parameters.\n",
    "\n",
    "**Required fields:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`security`|`String`|-|Security we want to get intra day tick from|\n",
    "|`startDateTime`|`Date`|-|Starting date we want to get tick from, formatted as YYYY-MM-dd HH:mm:ss|\n",
    "\n",
    "**Following options are supported:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`endDateTime`|`Date`|`NOW`|Ending date we want to get tick to, formatted as YYYY-MM-dd HH:mm:ss|\n",
    "|`eventTypes`|`List[String]`|`EMPTY`|Must be valid event types such as `TRADE`, `BID`, `ASK`, `SETTLE`, etc.|\n",
    "|`returnEids`|`Option[Boolean]`|`NONE`||\n",
    "|`includeConditionCodes`|`Option[Boolean]`|`NONE`|optionally include special trade flags like auctions|\n",
    "|`includeExchangeCodes`|`Option[Boolean]`|`NONE`||\n",
    "|`includeNonPlottableEvents`|`Option[Boolean]`|`NONE`||\n",
    "|`includeBrokerCodes`|`Option[Boolean]`|`NONE`||\n",
    "|`includeRpsCodes`|`Option[Boolean]`|`NONE`||\n",
    "|`includeBicMicCodes`|`Option[Boolean]`|`NONE`||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e770c28-6d75-4f4e-8f08-06fa55de585a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given distributed nature of spark, one can specify the number of partitions we want to distribute this request against. In this case, each partition will be responsible for a specific B-Pipe request against a given time window between `startDate` and `endDate` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b69ada1-0dbd-4332-9686-cefcc4f6a152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tick_df = (\n",
    "  spark\n",
    "    .read\n",
    "    .format(\"//blp/refdata\")\n",
    "    .option(\"serviceName\", \"IntradayTickRequest\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"security\", securities[0])\n",
    "    .option(\"returnEids\", True)\n",
    "    .option(\"includeNonPlottableEvents\", True)\n",
    "    .option(\"startDateTime\", \"2022-11-01\")\n",
    "    # partitioning by date\n",
    "    .option(\"partitions\", 10)\n",
    "    .load().toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6243493c-01ee-45a3-b3f3-62586170eaea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(tick_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bff672f-c5a4-4b62-9bb8-e877cecc8335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 4), tight_layout=True, dpi=200)\n",
    "plt.xlabel(\"TIME\")\n",
    "plt.ylabel(\"TRADE\")\n",
    "plt.title(securities[0])\n",
    "tick_df = tick_df.head(100)\n",
    "axs.plot(tick_df['TIME'],tick_df['VALUE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4453154-1931-41bb-8cd7-e363dc32638b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Intraday Bar Request\n",
    "\n",
    "IntradayBarRequest is an API request (using BLPAPI) that asks for time-aggregated market data instead of individual ticks — each “bar” summarizes trading over a fixed interval. Each bar typically contains time (timestamp of the bar start), open (first trade price in the interval), high (highest trade price), low (lowest trade price), close (last trade price), volume (total volume traded in the interval) and numEvents (number of ticks that occurred during the interval). We mapped its service again spark application accepting below parameters.\n",
    "\n",
    "**Required fields:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`security`|`String`|-|Security we want to get intra day tick from|\n",
    "|`startDateTime`|`Date`|-|Starting date we want to get tick from, formatted as YYYY-MM-dd HH:mm:ss|\n",
    "|`interval`|`Int`|-|Interval needs to be between 1 and 1440 minutes|\n",
    "\n",
    "**Following options are supported:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`endDateTime`|`Date`|`NOW`|Ending date we want to get tick to, formatted as YYYY-MM-dd HH:mm:ss|\n",
    "|`eventType`|`Option[String]`|`NONE`|Must be valid event type such as `TRADE`, `BID`, `ASK`, `SETTLE`, etc.|\n",
    "|`returnEids`|`Option[Boolean]`|`NONE`||\n",
    "|`gapFillInitialBar`|`Option[Boolean]`|`NONE`||\n",
    "|`adjustmentNormal`|`Option[Boolean]`|`NONE`||\n",
    "|`adjustmentAbnormal`|`Option[Boolean]`|`NONE`||\n",
    "|`adjustmentSplit`|`Option[Boolean]`|`NONE`||\n",
    "|`adjustmentFollowDPDF`|`Option[Boolean]`|`NONE`||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17234b9f-a6e0-43c7-a95f-f80293e483ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given distributed nature of spark, one can specify the number of partitions we want to distribute this request against. In this case, each partition will be responsible for a specific B-Pipe request against a given time window between `startDate` and `endDate` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fb3de3-0853-4b7b-812d-278577f07794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bar_df = (\n",
    "  spark\n",
    "    .read\n",
    "    .format(\"//blp/refdata\")\n",
    "    .option(\"serviceName\", \"IntradayBarRequest\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"interval\", 1000)\n",
    "    .option(\"security\", securities[0])\n",
    "    .option(\"startDateTime\", \"2022-11-01\")\n",
    "    # partitioning by date\n",
    "    .option(\"partitions\", 10)\n",
    "    .load().toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5022d1-e5cf-4f83-9659-163bff4c743a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(bar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50453542-f246-47f6-8918-e3f4516987e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# include candlestick with rangeselector\n",
    "bar_df = bar_df.head(200)\n",
    "fig.add_trace(\n",
    "  go.Candlestick(x=bar_df['TIME'],\n",
    "  open=bar_df['OPEN'], high=bar_df['HIGH'],\n",
    "  low=bar_df['LOW'], close=bar_df['OPEN']),\n",
    "  secondary_y=True)\n",
    "\n",
    "# include a go.Bar trace for volumes\n",
    "fig.add_trace(\n",
    "  go.Bar(x=bar_df['TIME'], y=bar_df['VOLUME']),\n",
    "  secondary_y=False)\n",
    "\n",
    "fig.layout.yaxis2.showgrid=False\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767f3e2d-bf0c-41b9-b254-59e65cbc08bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Market Data\n",
    "`//blp/mktdata` is the Bloomberg API service that streams real-time market data (quotes, trades, market depth) to client applications by subscription. We mapped its service again spark application accepting below parameters.\n",
    "\n",
    "**Required fields:**\n",
    "\n",
    "|Option|Type|Default|Description|\n",
    "|---|---|---|---|\n",
    "|`fields`|`List[String]`|-|fields we want to return for our given securities. See list of supported field from [DATA\\<GO\\>](https://data.bloomberg.com/)|\n",
    "|`securities`|`List[String]`|-|list of securities|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a254bc3-62b7-426d-88b3-03846f6b3f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Given distributed nature of spark, one can specify the number of partitions we want to distribute this request against. In this case, each partition will be responsible for a specific B-Pipe request against a subset of securities provided. Suboptimal in specific cases where portfolio is made of securities of different liquidity (different traded volumes), this might remove the bottleneck of streaming an entire portfolio through 1 single request. See next notebook for more information about imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bf709e-4c83-4f86-bd99-589d41f28705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "market_df = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .format(\"//blp/mktdata\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"fields\", \"['BID','ASK','TRADE_UPDATE_STAMP_RT']\")\n",
    "    .option(\"securities\", \"['SPY US EQUITY','MSFT US EQUITY']\")\n",
    "    # naive partitioning by security\n",
    "    # see next notebook for more advanced partitioning logic\n",
    "    .option(\"partitions\", 10)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "market_sq = (\n",
    "  market_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")         # memory = store in-memory table (for testing only)\n",
    "    .queryName(\"market_data\") # counts = name of the in-memory table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bedc636b-c708-4598-bcfc-9773f166662e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632e0b4b-327b-4726-89b3-23bcb3818267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table('market_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb160cf-894e-4970-b644-d5ded633ad32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "market_sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51f239b9-bc93-4194-b631-1f8118c704ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8279284259244383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_bpipe_reader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
