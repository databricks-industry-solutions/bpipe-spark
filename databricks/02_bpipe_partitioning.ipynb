{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a7ff50b-f0c4-4eeb-81ce-2434c297182c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# B-Pipe Smart Partitioning\n",
    "Since B-Pipe data cannot be load balanced against multiple servers, one may need to explicitly split queries into individual requests where each machine will handle different set of securities. However, some securities are overrepresented resulting in suboptimal partitioning. While some machines will struggle to cope up with volume of trades, others will remain mostly idle. But what if we could know in advance the expected volume of trades and split requests accordingly? Can we minimize variance across multiple partitions to ensure a more scalable and cost-effective solution? \n",
    "\n",
    "For the purpose of this exercise, let's access synthetic trade volumes for 900 random securities we want to pull out of B-Pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ddeae7-2e4c-46a0-93ff-cf49f6accac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "securities_stats = pd.read_csv('data/security_volume.csv')\n",
    "bpipe_securities = json.dumps(securities_stats.security.to_list())\n",
    "display(securities_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1871891d-cbde-4c98-9890-70c92917a7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As expected, some securities may be over-represented (exhibit a power of law distribution, better visualized\n",
    "in log scale below). Such a distribution will make partitioning tricky when distributing requests against multiple\n",
    "executors. During our observation window, the vast majority of securities resulted in ~ 10,000 trades whilst some\n",
    "were observed more than 10 million times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db8bf7c-f654-4ae1-a200-5a4d5f9c10bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "x = securities_stats['volume']\n",
    "_, bins = np.histogram(np.log10(x + 1), bins='auto')\n",
    "fig, axs = plt.subplots(1,1,figsize=(10, 7),tight_layout=True,dpi=100)\n",
    "\n",
    "# Remove axes splines\n",
    "for s in ['top', 'bottom', 'left', 'right']:\n",
    "    axs.spines[s].set_visible(False)\n",
    "\n",
    "# Remove x, y ticks\n",
    "axs.xaxis.set_ticks_position('none')\n",
    "axs.yaxis.set_ticks_position('none')\n",
    "   \n",
    "# Add padding between axes and labels\n",
    "axs.xaxis.set_tick_params(pad = 5)\n",
    "axs.yaxis.set_tick_params(pad = 10)\n",
    "\n",
    "# Compute histogram\n",
    "N, bins, patches = axs.hist(x, bins=10**bins, rwidth=0.9)\n",
    "\n",
    "# Setting color\n",
    "fracs = ((N**(1 / 5)) / N.max())\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "\n",
    "# Adding extra features   \n",
    "plt.xlabel(\"trades\")\n",
    "plt.ylabel(\"securities\")\n",
    "plt.title('Volume of trades for securities (log scale)')\n",
    "\n",
    "# Plot\n",
    "plt.gca().set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525b0651-35df-4ed0-8d5e-717f67c63bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's start by randomly assigning a given partition for each given security. This will give us an indication of the actual over / under utilization of each machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2541e28-a4c3-49ae-a6b7-babd27000d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_partitions = 10\n",
    "securities_stats['partition'] = [np.random.randint(0, num_partitions) for k in securities_stats.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e84e66c-c147-424b-bb31-430aa4e2e4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Graph below exhibits a clear imbalance of input data. Whilst some machines will be over utilized, other won't,\n",
    "resulting in a non-scalable and cost inefficient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a26c21-f07a-4051-8e9d-0858d4aa562a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trades_by_partition(df, file=None, dpi=100, y_lim=None):\n",
    "\n",
    "  x = df.index\n",
    "  y = df['sum']\n",
    "  mu = np.average(y)\n",
    "  sigma = np.std(y)\n",
    "  \n",
    "  fig, axs = plt.subplots(1,1,figsize=(15, 7),tight_layout=True,dpi=dpi)\n",
    "\n",
    "  # Remove axes splines\n",
    "  for s in ['top', 'bottom', 'left', 'right']:\n",
    "      axs.spines[s].set_visible(False)\n",
    "\n",
    "  # Remove x, y ticks\n",
    "  axs.xaxis.set_ticks_position('none')\n",
    "  axs.yaxis.set_ticks_position('none')\n",
    "\n",
    "  # Add padding between axes and labels\n",
    "  axs.xaxis.set_tick_params(pad = 5)\n",
    "  axs.yaxis.set_tick_params(pad = 10)\n",
    "  \n",
    "  # Plot\n",
    "  axs.bar(x, y, color = 'green', alpha = 0.5)\n",
    "  axs.hlines(mu, xmin=0, xmax=num_partitions, color='coral', ls='--', lw=0.8)\n",
    "  if y_lim:\n",
    "    axs.set_ylim(0, y_lim)\n",
    "\n",
    "  plt.xlabel(\"Partition Id\")\n",
    "  plt.ylabel(\"Volume of trades\")\n",
    "  plt.title(r\"Volume of trades per partition: $\\mu={:.2f}$ $\\sigma={:.2f}$\".format(mu, sigma))\n",
    "  \n",
    "  if file:\n",
    "    plt.savefig(file)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332e6f94-8d8b-41fc-8b0d-103d5933c83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_partitions = securities_stats.groupby('partition')['volume'].agg(['sum','count'])\n",
    "trades_by_partition(grouped_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aa28ee-b42a-4e13-afd3-ca4bc222c008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize partitions\n",
    "Optimal partitioning can be learned for our requested securities following  ML best practices. Leveraging mlflow + [hyperopt](https://docs.databricks.com/machine-learning/automl-hyperparam-tuning/index.html), we can group different set of securities together to minimize variance across partitions so that each and every machine we spin up will be worth the while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69729939-62cf-4703-8e91-0ed249a859eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "securities = securities_stats.set_index('security')['volume'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadd9b7b-2ab8-4ac4-856b-39ce263427cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We define our objective function (our loss that we want to minimize) as the total variance of the\n",
    "distribution of our trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85961fd-0ad3-41cf-990e-65f7bf7877c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import hp, fmin, tpe, SparkTrials, STATUS_OK, space_eval\n",
    "import mlflow\n",
    "\n",
    "def objective_function(partitions):\n",
    "  \n",
    "  # retrieve volume of trades by security\n",
    "  df = pd.DataFrame([[partitions[key], securities[key]] for key in securities.keys()], columns=['partition', 'volume'])\n",
    "  \n",
    "  # compute variance for this suggested partitioning\n",
    "  grouped_df = df.groupby('partition')['volume'].agg(['sum','count'])\n",
    "  var = np.var(grouped_df['sum'])\n",
    "\n",
    "  # our objective is to minimize variance of volume of trade by partition\n",
    "  return {'loss': var, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d292052-a1a3-4d0a-882d-dbae86010ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "... And define our optimizer where each security can randomly select one of our 10 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c700bc-dcc8-4fda-81d9-d83a052a50ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_space = {}\n",
    "for security in securities.keys():\n",
    "    search_space[security] = hp.randint(security, num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8daab42-7d12-492a-b4b9-a90ed1c78900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We track progress using mlflow and ensure parallel execution of our optimizer using SparkTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd987b2-b270-419b-b69a-4def6ecc4d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='bpipe_partitioning', nested=True) as run:\n",
    "  \n",
    "  argmin = fmin(\n",
    "    fn=objective_function,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest, \n",
    "    max_evals=10, # we can limit the number of combinations to evaluate\n",
    "    trials=SparkTrials(parallelism=10),  # we have 10 workers available for this task\n",
    "    verbose=True\n",
    "  )\n",
    "  \n",
    "  run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c96878c-3823-48b4-a541-4ca4e2844fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, after X number of runs, we can access the best set of parameters (i.e. the ideal partition number) for each security we want to ingest through B-Pipe. Whilst some large volume securities may deserve their own executors, other may be bundled together to ensure effective parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac529e53-0612-466c-b139-f483e748002e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_params = space_eval(search_space, argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d64fce-9d9c-4c08-932c-f682f32e0270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "distributed_securities = pd.DataFrame(\n",
    "  [[key, best_params[key], securities[key]] for key in securities.keys()],\n",
    "  columns=['security', 'partition', 'volume']\n",
    ")\n",
    "distributed_securities.index = securities.keys()\n",
    "display(distributed_securities.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f29ce2-9c66-4952-834c-4a36045efed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Not obvious to the untrained eyes given the log scale of our plot, our optimization **reduced our initial\n",
    "variance by 60%** (i.e. the imbalance) in expected throughput across all our 10 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb499e8-6407-407e-914a-b5dffe13b862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimized_partitions = distributed_securities.groupby('partition')['volume'].agg(['sum','count'])\n",
    "trades_by_partition(optimized_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473d1fbd-cb0d-4cd3-91cf-71f1767acba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Partition reference data\n",
    "Our spark wrapper can accept our partitioning logic through the partitions option (taking a list of partitions as JSON array). In this case, we are explicitly indicating what security will be read from what partition in the form of the following spark options:\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    ".option(\"securities\", \"['sec1', 'sec2']\")\n",
    ".option(\"partitions\", \"[1,2]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a0cda7-6874-4b90-8607-70a9b3a46f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "bpipe_securities = json.dumps(distributed_securities.security.to_list())\n",
    "bpipe_partitions = json.dumps(distributed_securities.partition.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf5f8cb-774d-48c3-a230-a014047acab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ref_data = (\n",
    "  spark\n",
    "    .read\n",
    "    .format(\"//blp/refdata\")\n",
    "    .option(\"serviceName\", \"ReferenceDataRequest\")\n",
    "    .option(\"serviceHost\", \"127.0.0.1\")\n",
    "    .option(\"servicePort\", 8954)\n",
    "    .option(\"correlationId\", 999)\n",
    "    .option(\"fields\", \"['BID','ASK']\")\n",
    "    # our list of securities\n",
    "    .option(\"securities\", bpipe_securities)\n",
    "    # our smart partitioning\n",
    "    .option(\"partitions\", bpipe_partitions) \n",
    "    .load()\n",
    ")\n",
    "\n",
    "display(ref_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e036694-939f-4cb6-8e13-18188719ccec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we showed how to efficiently read from B-Pipe data and leverage the distributed nature of spark to create a resilient and cost efficient ingestion framework that feeds into downstream spark / delta based applications."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_bpipe_partitioning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
